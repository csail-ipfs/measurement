# How do I create a network of IPFS vantage points from scratch?

If you are using an existing virtual machine template with IPFS dependencies installed, skip to 3.

## 1. Setting up the infrastructure

1. Spin up a remotely accessible MongoDB instance with password authentication enabled.

	a. Create a database named `ipfs` on the MongoDB instance.

	b. Edit `mongo_helpers` and insert the server's IP address, username, and password.
	
2. Create a new external/network-attached/etc. NFS volume to store the large temporary files generated by the measurement process.

	a. Edit `autorun-provision-node` and set `NFS_VOLUME` to the volume's IP and mount point.

3. Go to GitHub and generate a password authentication token with access to the ipfs-measuement repository. 

	a. Edit `autorun-provision-node` and set `GIT_USER` and `GIT_TOKEN` to your GitHub username and token.

## 2. Setting up the vantage point template

4. Create a new Debian or Ubuntu virtual machine with a minimum of 10 GB of disk space and 2 GB of RAM. 

5. SSH into the virtual machine and update the package manager database (`sudo apt update -y`)

6. Use `scp` or another file transfer tool to move `autorun-provision-node` to the new VM.

7. Image the VM, turn it into a template, and set `autorun-provision-node` to automatically run on boot.

8. Provision as many nodes as you need.

## 3. Running measurement

9. Upon start, nodes should automatically install all necessary software and start IPFS. 

10. You can check the status of your nodes by running `monitoring-tools/get-status.py`.

	a. Be sure to set the configuration details of your MongoDB instance up top in the script.

	b. You can also manually check on the provisioning of nodes by looking at the `ipfsconfig` collection in MongoDB. Each node will dump its configuration to this collection when provisioning is successful.
	
# Winding down and extracting data

Unfortunately, I haven't built an automated way to shut down each instance, so you'll have to SSH to each host and kill the `screen` instances with the labels `dht`, `ipfs`, and `logging`.

## Importing DHT data
DHT data is collected locally due to its size and thoroughput, so it will need to be manually imported into the database. Since all DHT data is collected onto the NFS mountpoint, you can SSH into any vantage point (or connect to the NFS disk locally). You can use the `mongoimport` tool for this:

	source $HOME/measurement/mongo_helpers
	cd $HOME/dht-data
	for i in *.log; do mimport --collection dht --file $i; done
	
## Extracting DHT data subset
Since the DHT data is large, it's best to do processing and subsetting on the MongoDB server before processing it locally. The `extract_dht_*.js` files can be run on a local MongoDB client and will create new collections on the MongoDB instance, which can then be queried or exported with `mongoexport`.

## Ping data
If you want to collect ping or latency data (which is not automatically done), you can run the `ping.py` script on your vantage point(s). There are two functions available: `pingSubsetList()` and `pingMyOldPeers()`. 

`pingSubsetList()` takes no arguments and expects a file named `peersubset.txt` in the same folder. It will ping all of the peers listed in this newline-delimited file.

`pingMyOldPeers(n)` takes one argument and samples the peers seen from this specific vantage point as of `n` hours ago. This is good for testing both latency and longevity of the peer network.

Results are stored in the same location as DHT data in file names with the format `"ping.{vp}.{time}"`. These files can be directly imported to MongoDB:

	source $HOME/measurement/mongo_helpers
	cd $HOME/dht-data
	for i in ping.*; do mimport --collection ping --file $i; done
	
# Analysis
 
Analyses and figure generation is detailed in the IPython notebooks located in the `analysis` folder. 

## Old-format IPFS logs

This shouldn't be an issue if you're running this from scratch, but the `importers` folder has some code for importing older logs. 



